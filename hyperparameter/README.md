# Hyperparameters

## Learning Rate
[Detailed Explaination Video](https://www.youtube.com/watch?v=HLMjeDez7ps)

### Adaptive Learning Optimizers
- AdamOptimizer
- AdagradOptimizer

### Minibatch Size
[Detailed Explaination Video](https://www.youtube.com/watch?v=GrrO1NFxaW8)

Pass in a batch of training examples to train together at a time to speed up training time. Has trade off with memory. Small batch size has more noise to stop finding local maximum, but could be too slow.

### Epochs
[Detailed Explaination Video](https://www.youtube.com/watch?v=TTdHpSb4DV8)

The number of iterations we should train. The metric to use to determine when to stop is to use validation error. For example, when validation has not exceed some amount for a number of iterations.

### Number of Hidden Units and Layers
[Detailed Explaination Video](https://www.youtube.com/watch?v=IkGAIQH5wH8)

If overfitting, can try to decrease number of hidden units. Keep adding hidden units, until validation error gets worse. 

In practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers)

### RNN Hyperparameters
[Detailed Explaination Video](https://www.youtube.com/watch?v=yQvnv7l_aUo)

- Cell type (GRU or LSTM)
- Layers 
- Size 
- Learning Rate
- Embedding Size
